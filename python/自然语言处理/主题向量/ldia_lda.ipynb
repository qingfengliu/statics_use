{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf0f90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\users\\50477\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pugnlp\\constants.py:136: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  [datetime.datetime, pd.datetime, pd.Timestamp])\n",
      "d:\\users\\50477\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pugnlp\\constants.py:158: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n",
      "d:\\users\\50477\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pugnlp\\tutil.py:100: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "d:\\users\\50477\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pugnlp\\util.py:80: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "INFO:nlpia.constants:Starting logger in nlpia.constants...\n",
      "d:\\users\\50477\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nlpia\\futil.py:30: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "d:\\users\\50477\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nlpia\\loaders.py:78: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "INFO:nlpia.loaders:No BIGDATA index found in d:\\users\\50477\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv so copy d:\\users\\50477\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nlpia\\data\\bigdata_info.latest.csv to d:\\users\\50477\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv if you want to \"freeze\" it.\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('d:\\\\users\\\\50477\\\\appdata\\\\local\\\\programs\\\\python\\\\python38\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\mavis-batey-greetings.csv',), **{'low_memory': False})`...\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('d:\\\\users\\\\50477\\\\appdata\\\\local\\\\programs\\\\python\\\\python38\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\sms-spam.csv',), **{'low_memory': False})`...\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('d:\\\\users\\\\50477\\\\appdata\\\\local\\\\programs\\\\python\\\\python38\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\sms-spam.csv',), **{'nrows': None, 'low_memory': False})`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       spam                                               text\n",
      "sms0      0  Go until jurong point, crazy.. Available only ...\n",
      "sms1      0                      Ok lar... Joking wif u oni...\n",
      "sms2!     1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "sms3      0  U dun say so early hor... U c already then say...\n",
      "sms4      0  Nah I don't think he goes to usf, he lives aro...\n",
      "sms5!     1  FreeMsg Hey there darling it's been 3 week's n...\n",
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      ",            1\n",
      "..           1\n",
      "...          2\n",
      "amore        1\n",
      "available    1\n",
      "Name: sms0, dtype: int64\n",
      "(16, 9232)\n",
      "   topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  \\\n",
      "!  184.03   15.00   72.22  394.95   45.48   36.14    9.55   44.81   \n",
      "\"    0.68    4.22    2.41    0.06  152.35    0.06    0.06    0.06   \n",
      "#    0.06    0.06    0.06    0.06    0.06    2.07    0.06    0.06   \n",
      "\n",
      "   topic8  topic9  topic10  topic11  topic12  topic13  topic14  topic15  \n",
      "!    0.43   90.23    37.42    44.18    64.40   297.29    41.16    11.70  \n",
      "\"    0.45    0.68     8.42    11.42     0.07    62.72    12.27     0.06  \n",
      "#    0.06    0.06     0.06     0.06     1.07     4.05     0.06     0.06  \n",
      "!       394.952246\n",
      ".       218.049724\n",
      "to      119.533134\n",
      "u       118.857546\n",
      "call    111.948541\n",
      "£       107.358914\n",
      ",        96.954384\n",
      "*        90.314783\n",
      "your     90.215961\n",
      "is       75.750037\n",
      "Name: topic3, dtype: float64\n",
      "       topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  \\\n",
      "sms0     0.00    0.62    0.00    0.00    0.00    0.00    0.00    0.00   \n",
      "sms1     0.01    0.01    0.01    0.01    0.01    0.01    0.01    0.01   \n",
      "sms2!    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   \n",
      "sms3     0.00    0.00    0.00    0.00    0.09    0.00    0.00    0.00   \n",
      "sms4     0.39    0.00    0.33    0.00    0.00    0.00    0.14    0.00   \n",
      "\n",
      "       topic8  topic9  topic10  topic11  topic12  topic13  topic14  \\\n",
      "sms0     0.34    0.00     0.00     0.00     0.00     0.00     0.00   \n",
      "sms1     0.78    0.01     0.01     0.12     0.01     0.01     0.01   \n",
      "sms2!    0.00    0.98     0.00     0.00     0.00     0.00     0.00   \n",
      "sms3     0.85    0.00     0.00     0.00     0.00     0.00     0.00   \n",
      "sms4     0.00    0.00     0.00     0.00     0.09     0.00     0.00   \n",
      "\n",
      "       topic15  \n",
      "sms0      0.00  \n",
      "sms1      0.01  \n",
      "sms2!     0.00  \n",
      "sms3      0.00  \n",
      "sms4      0.00  \n",
      "0.94\n",
      "1.0\n",
      "0.748\n",
      "(32, 9232)\n",
      "       topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  \\\n",
      "sms0      0.0    0.00     0.0    0.06    0.14    0.00     0.0     0.0   \n",
      "sms1      0.0    0.00     0.0    0.00    0.53    0.00     0.0     0.0   \n",
      "sms2!     0.0    0.00     0.0    0.00    0.00    0.65     0.0     0.0   \n",
      "sms3      0.0    0.11     0.0    0.00    0.39    0.00     0.0     0.0   \n",
      "sms4      0.0    0.00     0.0    0.00    0.00    0.00     0.0     0.0   \n",
      "\n",
      "       topic8  topic9  ...  topic22  topic23  topic24  topic25  topic26  \\\n",
      "sms0      0.0     0.0  ...      0.0      0.0     0.00     0.00      0.0   \n",
      "sms1      0.0     0.0  ...      0.0      0.0     0.00     0.00      0.0   \n",
      "sms2!     0.0     0.0  ...      0.0      0.0     0.00     0.33      0.0   \n",
      "sms3      0.0     0.0  ...      0.0      0.0     0.00     0.00      0.0   \n",
      "sms4      0.0     0.0  ...      0.0      0.0     0.09     0.00      0.0   \n",
      "\n",
      "       topic27  topic28  topic29  topic30  topic31  \n",
      "sms0      0.00      0.0     0.00      0.0      0.0  \n",
      "sms1      0.00      0.0     0.14      0.0      0.0  \n",
      "sms2!     0.00      0.0     0.00      0.0      0.0  \n",
      "sms3      0.00      0.0     0.00      0.0      0.0  \n",
      "sms4      0.47      0.0     0.00      0.0      0.0  \n",
      "\n",
      "[5 rows x 32 columns]\n",
      "(2418, 32)\n",
      "0.933\n",
      "0.936\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nlpia.data.loaders import get_data\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDiA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "\n",
    "# 从 nlpia 包中的 DataFrame 加载短消息数据\n",
    "pd.options.display.width = 120\n",
    "sms = get_data('sms-spam')\n",
    "# 向短消息的索引号后面添加一个感叹号，以使垃圾短消息更容易被发现\n",
    "index = ['sms{}{}'.format(i, '!'*j) for (i,j) in zip(range(len(sms)), sms.spam)]\n",
    "sms.index = index\n",
    "print(sms.head(6))\n",
    "\n",
    "# 计算词袋向量\n",
    "np.random.seed(42)\n",
    "counter = CountVectorizer(tokenizer=casual_tokenize)\n",
    "bow_docs = pd.DataFrame(counter.fit_transform(raw_documents=sms.text).toarray(), index=index)\n",
    "column_nums, terms = zip(*sorted(zip(counter.vocabulary_.values(), counter.vocabulary_.keys())))\n",
    "bow_docs.columns = terms\n",
    "# 看看对标记为“sms0”的第一条短消息\n",
    "print(sms.loc['sms0'].text)\n",
    "print( bow_docs.loc['sms0'][bow_docs.loc['sms0'] > 0].head())\n",
    "\n",
    "ldia = LDiA(n_components=16, learning_method='batch')\n",
    "ldia = ldia.fit(bow_docs)\n",
    "# 将 9232 个词（词项）分配给 16 个主题（成分）\n",
    "print(ldia.components_.shape)\n",
    "\n",
    "# 看看开头的几个词，我们了解一下它们是如何分配到 16 个主题中的。\n",
    "pd.set_option('display.width', 75)\n",
    "columns = ['topic{}'.format(i) for i in range(ldia.n_components)]\n",
    "components = pd.DataFrame(ldia.components_.T, index=terms, columns=columns)\n",
    "# 感叹号（!）被分配到大多数主题中，但它其实是 topic3 中一个特别重要的部分，\n",
    "# 在该主题中引号（\"）几乎不起作用。\n",
    "print(components.round(2).head(3))\n",
    "# 该主题的前十个词条似乎是在要求某人做某事或支付某事的强调指令中可能使用的词类型。\n",
    "print(components.topic3.sort_values(ascending=False)[:10])\n",
    "\n",
    "# 生成主题向量\n",
    "ldia16_topic_vectors = ldia.transform(bow_docs)\n",
    "ldia16_topic_vectors = pd.DataFrame(ldia16_topic_vectors, index=index, columns=columns)\n",
    "# 对比于pca,svd，ldia产生的主题之间分隔得更加清晰\n",
    "print(ldia16_topic_vectors.round(2).head())\n",
    "\n",
    "# lda\n",
    "X_train, X_test, y_train, y_test = train_test_split(ldia16_topic_vectors, sms.spam, test_size=0.5, random_state=271828)\n",
    "lda = LDA(n_components=1)\n",
    "'''\n",
    "ldia_topic_vectors 矩阵的行列式接近于零，所以很可能会得到\n",
    "“变量是共线的”这类警告。这种情况可能发生在小型语料库\n",
    "上使用 LDiA 的场景，因为这时的主题向量中有很多 0，并且\n",
    "一些消息可以被重新生成为其他消息主题的线性组合。另一\n",
    "种可能的场景是，语料库中有一些具有相似（或相同）主题\n",
    "混合的短消息。共线警告可能发生的一种情况是，如果文本包含一些 2-gram 或 3-gram，\n",
    "其中组成它们的词只同时出现在这些 2-gram 或 3-gram 中。\n",
    "'''\n",
    "lda = lda.fit(X_train, y_train)\n",
    "sms['ldia16_spam'] = lda.predict(ldia16_topic_vectors)\n",
    "# 测试集上取得 94%的精确度\n",
    "print(round(float(lda.score(X_test, y_test)), 2))\n",
    "\n",
    "# 看看与基于 TF-IDF 向量的高维模型相比结果如何\n",
    "tfidf = TfidfVectorizer(tokenizer=casual_tokenize)\n",
    "tfidf_docs = tfidf.fit_transform(raw_documents=sms.text).toarray()\n",
    "tfidf_docs = tfidf_docs - tfidf_docs.mean(axis=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_docs, sms.spam.values, test_size=0.5, random_state=271828)\n",
    "# “假装”所有短消息中都只有一个主题\n",
    "lda = LDA(n_components=1)\n",
    "# 它用一个 9232维的超平面分割向量空间！\n",
    "lda = lda.fit(X_train, y_train)\n",
    "print(round(float(lda.score(X_train, y_train)), 3))\n",
    "# TF-IDF 向量有更多的特征（超过 3000 个独立的词项）。所以很可能会遇到过拟合和弱泛化问题\n",
    "print(round(float(lda.score(X_test, y_test)), 3))\n",
    "\n",
    "# 增加主题数量，看看主题向量中0是否会减少：32 个 LDiA 主题\n",
    "ldia32 = LDiA(n_components=32, learning_method='batch')\n",
    "ldia32 = ldia32.fit(bow_docs)\n",
    "print(ldia32.components_.shape)\n",
    "ldia32_topic_vectors = ldia32.transform(bow_docs)\n",
    "columns32 = ['topic{}'.format(i) for i in range(ldia32.n_components)]\n",
    "ldia32_topic_vectors = pd.DataFrame(ldia32_topic_vectors, index=index, columns=columns32)\n",
    "# 可以看到，这些主题甚至更加稀疏，而且能更加清晰地分隔开。因此增加或减少主题的数量并\n",
    "# 不能解决或造成共线问题，这是底层数据造成的。\n",
    "# 如果想摆脱这个警告，那么需要将“噪声”或元数据以人造词的方式添加到短消息中，或者需要删除那些重复的词向量。\n",
    "# 如果文档中有重复出现多次的词向量或词对，那么主题的数量优化也无法解决这个问题。\n",
    "print(ldia32_topic_vectors.round(2).head())\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split(ldia32_topic_vectors, sms.spam, test_size=0.5, random_state=271828)\n",
    "lda = LDA(n_components=1)\n",
    "lda = lda.fit(X_train, y_train)\n",
    "sms['ldia32_spam'] = lda.predict(ldia32_topic_vectors)\n",
    "print(X_train.shape)\n",
    "print(round(float(lda.score(X_train, y_train)), 3))\n",
    "# 这里 93.6%的测试结果与使用16 维 LDiA 主题向量时 94%的测试结果相当\n",
    "print(round(float(lda.score(X_test, y_test)), 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0349af82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
