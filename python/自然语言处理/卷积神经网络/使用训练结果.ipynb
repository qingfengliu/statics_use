{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d3ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "with open(\"D:/中间结果/cnn_model.json\", \"r\") as json_file:\n",
    "    json_string = json_file.read()\n",
    "model = model_from_json(json_string)\n",
    "\n",
    "model.load_weights('D:/中间结果/cnn_weights.h5')\n",
    "word_vectors = KeyedVectors.load_word2vec_format(r'D:\\数据集\\谷歌新闻word2dev\\GoogleNews-vectors-negative300.bin.gz',\n",
    "                                                 binary=True, limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1c509aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = \"I'm hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f080c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_vectorize(dataset):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenizer.tokenize(sample[1])\n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sample_vecs.append(word_vectors[token])\n",
    "            except KeyError:\n",
    "                pass  # No matching token in the Google w2v vocab\n",
    "\n",
    "        vectorized_data.append(sample_vecs)\n",
    "\n",
    "    return vectorized_data\n",
    "#将句子转换为词向量\n",
    "vec_list = tokenize_and_vectorize([(1, sample_1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da33e4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3995881]]\n"
     ]
    }
   ],
   "source": [
    "def pad_trunc(data, maxlen):\n",
    "    \"\"\" For a given dataset pad with zero vectors or truncate to maxlen \"\"\"\n",
    "    new_data = []\n",
    "\n",
    "    # Create a vector of 0's the length of our word vectors\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "\n",
    "    for sample in data:\n",
    "\n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "    return new_data\n",
    "maxlen = 400        #\n",
    "embedding_dims = 300    # 词向量的长度,影响神经网络结构形状\n",
    "test_vec_list = pad_trunc(vec_list, maxlen)\n",
    "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
    "#利用已知模型预测\n",
    "#可能由于更改了模型参数的原因,模型给出了不那么确定的结果,但是仍然可以说\n",
    "#是负向感情\n",
    "print(model.predict(test_vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de575ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
