{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9eda46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " 'got',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " ',',\n",
       " 'would',\n",
       " 'get',\n",
       " 'home',\n",
       " '.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import Counter\n",
    "sentence=\"\"\"The faster Harry got to the store,the faster Harry,the faster,would get home.\n",
    "\"\"\"\n",
    "#分词\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "tokens=tokenizer.tokenize(sentence.lower())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9065455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 4, 'faster': 3, ',': 3, 'harry': 2, 'got': 1, 'to': 1, 'store': 1, 'would': 1, 'get': 1, 'home': 1, '.': 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 4), ('faster', 3), (',', 3), ('harry', 2)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#统计词频（频率）\n",
    "bag_of_words=Counter(tokens)\n",
    "print(bag_of_words)\n",
    "\n",
    "#出现次数最多的四个词\n",
    "bag_of_words.most_common(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6421ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1818"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#单词harry词频(概率)\n",
    "times_harry_appears=bag_of_words['harry']\n",
    "num_unique_words=len(bag_of_words)\n",
    "tf=times_harry_appears/num_unique_words\n",
    "round(tf,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d6fb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\users\\50477\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pugnlp\\constants.py:136: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  [datetime.datetime, pd.datetime, pd.Timestamp])\n",
      "d:\\users\\50477\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pugnlp\\constants.py:158: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n",
      "d:\\users\\50477\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pugnlp\\tutil.py:100: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "d:\\users\\50477\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pugnlp\\util.py:80: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "INFO:nlpia.constants:Starting logger in nlpia.constants...\n",
      "d:\\users\\50477\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nlpia\\futil.py:30: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "d:\\users\\50477\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nlpia\\loaders.py:78: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "INFO:nlpia.loaders:No BIGDATA index found in d:\\users\\50477\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv so copy d:\\users\\50477\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nlpia\\data\\bigdata_info.latest.csv to d:\\users\\50477\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv if you want to \"freeze\" it.\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('d:\\\\users\\\\50477\\\\appdata\\\\local\\\\programs\\\\python\\\\python38\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\mavis-batey-greetings.csv',), **{'low_memory': False})`...\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('d:\\\\users\\\\50477\\\\appdata\\\\local\\\\programs\\\\python\\\\python38\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\sms-spam.csv',), **{'low_memory': False})`...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'a': 20,\n",
       "         'kite': 16,\n",
       "         'is': 7,\n",
       "         'traditionally': 1,\n",
       "         'tethered': 2,\n",
       "         'heavier-than-air': 1,\n",
       "         'craft': 2,\n",
       "         'with': 2,\n",
       "         'wing': 5,\n",
       "         'surfaces': 1,\n",
       "         'that': 2,\n",
       "         'react': 1,\n",
       "         'against': 1,\n",
       "         'the': 26,\n",
       "         'air': 2,\n",
       "         'to': 5,\n",
       "         'create': 1,\n",
       "         'lift': 4,\n",
       "         'and': 10,\n",
       "         'drag.': 1,\n",
       "         'consists': 2,\n",
       "         'of': 10,\n",
       "         'wings': 1,\n",
       "         ',': 15,\n",
       "         'tethers': 2,\n",
       "         'anchors.': 2,\n",
       "         'kites': 8,\n",
       "         'often': 2,\n",
       "         'have': 4,\n",
       "         'bridle': 2,\n",
       "         'guide': 1,\n",
       "         'face': 1,\n",
       "         'at': 3,\n",
       "         'correct': 1,\n",
       "         'angle': 1,\n",
       "         'so': 3,\n",
       "         'wind': 2,\n",
       "         'can': 3,\n",
       "         'it.': 1,\n",
       "         \"'s\": 2,\n",
       "         'also': 3,\n",
       "         'may': 4,\n",
       "         'be': 5,\n",
       "         'designed': 2,\n",
       "         'not': 1,\n",
       "         'needed': 1,\n",
       "         ';': 2,\n",
       "         'when': 2,\n",
       "         'kiting': 3,\n",
       "         'sailplane': 1,\n",
       "         'for': 2,\n",
       "         'launch': 1,\n",
       "         'tether': 1,\n",
       "         'meets': 1,\n",
       "         'single': 1,\n",
       "         'point.': 1,\n",
       "         'fixed': 1,\n",
       "         'or': 6,\n",
       "         'moving': 2,\n",
       "         'untraditionally': 1,\n",
       "         'in': 7,\n",
       "         'technical': 2,\n",
       "         'tether-set-coupled': 1,\n",
       "         'sets': 1,\n",
       "         'even': 2,\n",
       "         'though': 1,\n",
       "         'system': 1,\n",
       "         'still': 1,\n",
       "         'called': 2,\n",
       "         'kite.': 1,\n",
       "         'sustains': 1,\n",
       "         'flight': 1,\n",
       "         'generated': 1,\n",
       "         'flows': 1,\n",
       "         'around': 1,\n",
       "         'surface': 2,\n",
       "         'producing': 1,\n",
       "         'low': 1,\n",
       "         'pressure': 2,\n",
       "         'above': 1,\n",
       "         'high': 1,\n",
       "         'below': 1,\n",
       "         'wings.': 1,\n",
       "         'interaction': 1,\n",
       "         'generates': 1,\n",
       "         'horizontal': 1,\n",
       "         'drag': 2,\n",
       "         'along': 1,\n",
       "         'direction': 1,\n",
       "         'wind.': 1,\n",
       "         'resultant': 1,\n",
       "         'force': 2,\n",
       "         'vector': 1,\n",
       "         'from': 1,\n",
       "         'components': 1,\n",
       "         'opposed': 1,\n",
       "         'by': 2,\n",
       "         'tension': 1,\n",
       "         'one': 1,\n",
       "         'more': 1,\n",
       "         'lines': 1,\n",
       "         'which': 2,\n",
       "         'attached.': 1,\n",
       "         'anchor': 1,\n",
       "         'point': 1,\n",
       "         'line': 1,\n",
       "         'static': 1,\n",
       "         '(': 1,\n",
       "         'e.g.': 1,\n",
       "         'towing': 1,\n",
       "         'running': 1,\n",
       "         'person': 1,\n",
       "         'boat': 1,\n",
       "         'free-falling': 1,\n",
       "         'anchors': 1,\n",
       "         'as': 5,\n",
       "         'paragliders': 1,\n",
       "         'fugitive': 1,\n",
       "         'parakites': 1,\n",
       "         'vehicle': 1,\n",
       "         ')': 1,\n",
       "         '.': 2,\n",
       "         'same': 1,\n",
       "         'principles': 1,\n",
       "         'fluid': 1,\n",
       "         'flow': 1,\n",
       "         'apply': 1,\n",
       "         'liquids': 1,\n",
       "         'are': 3,\n",
       "         'used': 2,\n",
       "         'under': 1,\n",
       "         'water.': 1,\n",
       "         'hybrid': 1,\n",
       "         'comprising': 1,\n",
       "         'both': 1,\n",
       "         'lighter-than-air': 1,\n",
       "         'balloon': 1,\n",
       "         'well': 1,\n",
       "         'lifting': 1,\n",
       "         'kytoon.': 1,\n",
       "         'long': 1,\n",
       "         'varied': 1,\n",
       "         'history': 1,\n",
       "         'many': 1,\n",
       "         'different': 1,\n",
       "         'types': 1,\n",
       "         'flown': 3,\n",
       "         'individually': 1,\n",
       "         'festivals': 1,\n",
       "         'worldwide.': 1,\n",
       "         'recreation': 1,\n",
       "         'art': 1,\n",
       "         'other': 1,\n",
       "         'practical': 1,\n",
       "         'uses.': 1,\n",
       "         'sport': 1,\n",
       "         'aerial': 1,\n",
       "         'ballet': 1,\n",
       "         'sometimes': 1,\n",
       "         'part': 1,\n",
       "         'competition.': 1,\n",
       "         'power': 2,\n",
       "         'multi-line': 1,\n",
       "         'steerable': 1,\n",
       "         'generate': 1,\n",
       "         'large': 1,\n",
       "         'forces': 1,\n",
       "         'activities': 1,\n",
       "         'such': 1,\n",
       "         'surfing': 1,\n",
       "         'landboarding': 1,\n",
       "         'fishing': 1,\n",
       "         'buggying': 1,\n",
       "         'new': 1,\n",
       "         'trend': 1,\n",
       "         'snow': 1,\n",
       "         'kiting.': 1,\n",
       "         'man-lifting': 1,\n",
       "         'been': 1,\n",
       "         'made': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#维基百科-风筝\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "from nlpia.data.loaders import kite_text\n",
    "tokens = tokenizer.tokenize(kite_text.lower())\n",
    "kite_counts=Counter(tokens)\n",
    "kite_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f5ad370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07162534435261708,\n",
       " 0.05509641873278237,\n",
       " 0.0440771349862259,\n",
       " 0.04132231404958678,\n",
       " 0.027548209366391185,\n",
       " 0.027548209366391185,\n",
       " 0.02203856749311295,\n",
       " 0.01928374655647383,\n",
       " 0.01928374655647383,\n",
       " 0.01652892561983471,\n",
       " 0.013774104683195593,\n",
       " 0.013774104683195593,\n",
       " 0.013774104683195593,\n",
       " 0.013774104683195593,\n",
       " 0.011019283746556474,\n",
       " 0.011019283746556474,\n",
       " 0.011019283746556474,\n",
       " 0.008264462809917356,\n",
       " 0.008264462809917356,\n",
       " 0.008264462809917356,\n",
       " 0.008264462809917356,\n",
       " 0.008264462809917356,\n",
       " 0.008264462809917356,\n",
       " 0.008264462809917356,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.005509641873278237,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185,\n",
       " 0.0027548209366391185]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#生成词频向量\n",
    "document_vector=[]\n",
    "doc_length=len(tokens)\n",
    "for key,value in kite_counts.most_common():\n",
    "    document_vector.append(value/doc_length)\n",
    "document_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c257e6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " 'and',\n",
       " 'as',\n",
       " 'faster',\n",
       " 'get',\n",
       " 'got',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'home',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'not',\n",
       " 'store',\n",
       " 'than',\n",
       " 'the',\n",
       " 'to',\n",
       " 'would']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=[\"The faster Harry got to the store,the faster and faster Harry would get home.\"]\n",
    "docs.append(\"Harry is hairy and faster than Jill\")\n",
    "docs.append(\"Jill is not as hairy as Harry\")\n",
    "\n",
    "doc_tokens=[]\n",
    "for doc in docs:\n",
    "    doc_tokens+=[sorted(tokenizer.tokenize(doc.lower()))]\n",
    "print(len(doc_tokens[0]))\n",
    "all_doc_tokens=sum(doc_tokens,[])\n",
    "len(all_doc_tokens)\n",
    "\n",
    "lexicon=sorted(set(all_doc_tokens))\n",
    "len(lexicon)\n",
    "\n",
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e94021b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(',', 0), ('.', 0), ('and', 0), ('as', 0), ('faster', 0), ('get', 0), ('got', 0), ('hairy', 0), ('harry', 0), ('home', 0), ('is', 0), ('jill', 0), ('not', 0), ('store', 0), ('than', 0), ('the', 0), ('to', 0), ('would', 0)])\n",
      "[OrderedDict([(',', 0.05555555555555555), ('.', 0.05555555555555555), ('and', 0.05555555555555555), ('as', 0), ('faster', 0.16666666666666666), ('get', 0.05555555555555555), ('got', 0.05555555555555555), ('hairy', 0), ('harry', 0.1111111111111111), ('home', 0.05555555555555555), ('is', 0), ('jill', 0), ('not', 0), ('store', 0.05555555555555555), ('than', 0), ('the', 0.16666666666666666), ('to', 0.05555555555555555), ('would', 0.05555555555555555)]), OrderedDict([(',', 0), ('.', 0), ('and', 0.05555555555555555), ('as', 0), ('faster', 0.05555555555555555), ('get', 0), ('got', 0), ('hairy', 0.05555555555555555), ('harry', 0.05555555555555555), ('home', 0), ('is', 0.05555555555555555), ('jill', 0.05555555555555555), ('not', 0), ('store', 0), ('than', 0.05555555555555555), ('the', 0), ('to', 0), ('would', 0)]), OrderedDict([(',', 0), ('.', 0), ('and', 0), ('as', 0.1111111111111111), ('faster', 0), ('get', 0), ('got', 0), ('hairy', 0.05555555555555555), ('harry', 0.05555555555555555), ('home', 0), ('is', 0.05555555555555555), ('jill', 0.05555555555555555), ('not', 0.05555555555555555), ('store', 0), ('than', 0), ('the', 0), ('to', 0), ('would', 0)])]\n"
     ]
    }
   ],
   "source": [
    "#将两个文章的词频融合在一起然后新的两个词频向量\n",
    "from collections import OrderedDict\n",
    "zero_vector=OrderedDict((token,0) for token in lexicon)\n",
    "print(zero_vector)\n",
    "\n",
    "import copy\n",
    "doc_vectors=[]\n",
    "for doc in docs:\n",
    "    vec=copy.copy(zero_vector)\n",
    "    tokens=tokenizer.tokenize(doc.lower())\n",
    "    token_counts=Counter(tokens)\n",
    "    for key,value in token_counts.items():\n",
    "        vec[key]=value/len(lexicon)\n",
    "    doc_vectors.append(vec)\n",
    "print(doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1011f9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363\n",
      "297\n"
     ]
    }
   ],
   "source": [
    "#主题分析先看两篇关于kite文档的词频情况\n",
    "from nlpia.data.loaders import kite_history,kite_text\n",
    "kite_intro=kite_text.lower()\n",
    "intro_tokens=tokenizer.tokenize(kite_intro)\n",
    "kite_history=kite_history.lower()\n",
    "history_tokens=tokenizer.tokenize(kite_history)\n",
    "intro_total=len(intro_tokens)\n",
    "print(intro_total)\n",
    "histoy_total=len(history_tokens)\n",
    "print(histoy_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd1c9a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency of \"kite\" in intro is:0.0441\n",
      "Term Frequency of \"kite\" in history is:0.0202\n",
      "Term Frequency of \"and\" in intro is:0.0441\n",
      "Term Frequency of \"and\" in history is:0.0202\n"
     ]
    }
   ],
   "source": [
    "intro_tf={}\n",
    "histroy_tf={}\n",
    "intro_counts=Counter(intro_tokens)\n",
    "intro_tf['kite']=intro_counts['kite']/intro_total\n",
    "history_counts=Counter(history_tokens)\n",
    "histroy_tf['kite']=history_counts['kite']/histoy_total\n",
    "print('Term Frequency of \"kite\" in intro is:{:.4f}'.format(intro_tf['kite']))\n",
    "print('Term Frequency of \"kite\" in history is:{:.4f}'.format(histroy_tf['kite']))\n",
    "\n",
    "intro_tf['and']=intro_counts['and']/intro_total\n",
    "histroy_tf['and']=history_counts['and']/histoy_total\n",
    "print('Term Frequency of \"and\" in intro is:{:.4f}'.format(intro_tf['kite']))\n",
    "print('Term Frequency of \"and\" in history is:{:.4f}'.format(histroy_tf['kite']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e2d964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tfidf_vectors=[]\n",
    "for doc in docs:\n",
    "    vec=copy.copy(zero_vector)\n",
    "    tokens=tokenizer.tokenize(doc.lower())\n",
    "    token_counts=Counter(tokens)\n",
    "    for key,value in token_counts.items():\n",
    "        docs_containing_key=0\n",
    "        for _doc in docs:\n",
    "            if key in _doc:\n",
    "                docs_containing_key+=1\n",
    "        tf=value/len(lexicon)\n",
    "        if docs_containing_key:\n",
    "            idf=len(docs)/docs_containing_key\n",
    "        else:\n",
    "            idf=0\n",
    "        vec[key]=tf*idf\n",
    "    document_tfidf_vectors.append(vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "930faab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([(',', 0.16666666666666666),\n",
       "              ('.', 0.16666666666666666),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.25),\n",
       "              ('get', 0.16666666666666666),\n",
       "              ('got', 0.16666666666666666),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0.16666666666666666),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.16666666666666666),\n",
       "              ('than', 0),\n",
       "              ('the', 0.5),\n",
       "              ('to', 0.16666666666666666),\n",
       "              ('would', 0.16666666666666666)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.08333333333333333),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('than', 0.16666666666666666),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0),\n",
       "              ('and', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0.16666666666666666),\n",
       "              ('store', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8bac7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'to': 2, 'how': 1, 'long': 1, 'does': 1, 'it': 1, 'take': 1, 'get': 1, 'the': 1, 'store': 1, '?': 1})\n",
      "18 18\n",
      "0.5991446895152779\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def consine_sim(vec1,vec2):\n",
    "    \"\"\"Let's convert our dictionaries to lists for easier matching.\"\"\"\n",
    "    vec1=[val for val in vec1.values()]\n",
    "    vec2=[val for val in vec2.values()]\n",
    "    \n",
    "    dot_prod=0\n",
    "    for i,v in enumerate(vec1):\n",
    "        dot_prod+=v*vec2[i]\n",
    "    mag_1=math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2=math.sqrt(sum([x**2 for x in vec2]))\n",
    "    return dot_prod/(mag_1*mag_2)\n",
    "\n",
    "#用相似度来实现对一个搜索的排序\n",
    "query=\"How long does it take to get to the store?\"\n",
    "query_vec=copy.copy(zero_vector)\n",
    "query_vec=copy.copy(zero_vector)\n",
    "tokens=tokenizer.tokenize(query.lower())\n",
    "token_counts=Counter(tokens)\n",
    "\n",
    "    \n",
    "print(token_counts)\n",
    "for key,value in token_counts.items():\n",
    "    docs_containing_key=0\n",
    "    \n",
    "    for _doc in docs:\n",
    "        if key in _doc.lower():\n",
    "            docs_containing_key+=1\n",
    "        \n",
    "    if docs_containing_key==0:\n",
    "        continue\n",
    "    tf=value/len(tokens)\n",
    "    idf=len(docs)/docs_containing_key\n",
    "    query_vec[key]=tf*idf\n",
    "        \n",
    "print(len(query_vec),len(document_tfidf_vectors[0]))\n",
    "print(consine_sim(query_vec,document_tfidf_vectors[0]))\n",
    "print(consine_sim(query_vec,document_tfidf_vectors[1]))\n",
    "print(consine_sim(query_vec,document_tfidf_vectors[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9eac084a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16 0.   0.48 0.21 0.21 0.   0.25 0.21 0.   0.   0.   0.21 0.   0.64\n",
      "  0.21 0.21]\n",
      " [0.37 0.   0.37 0.   0.   0.37 0.29 0.   0.37 0.37 0.   0.   0.49 0.\n",
      "  0.   0.  ]\n",
      " [0.   0.75 0.   0.   0.   0.29 0.22 0.   0.29 0.29 0.38 0.   0.   0.\n",
      "  0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "#使用TF-IDF矩阵\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus=docs\n",
    "vectorizer=TfidfVectorizer(min_df=1)\n",
    "model=vectorizer.fit_transform(corpus)\n",
    "print(model.todense().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe8d6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
