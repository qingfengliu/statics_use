{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb84aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load_word2vec_format(r'D:\\数据集\\谷歌新闻word2dev\\GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73556477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpia.data.loaders import get_data\n",
    "cities = get_data('cities')\n",
    "cities.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8dac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "us = cities[(cities.country_code == 'US') & (cities.admin1_code.notnull())].copy()\n",
    "states = pd.read_csv(r'D:\\数据集\\美国城市\\states.csv')\n",
    "states = dict(zip(states.Abbreviation, states.State))\n",
    "us['city'] = us.name.copy()\n",
    "us['st'] = us.admin1_code.copy()\n",
    "us['state'] = us.st.map(states)\n",
    "us[us.columns[-3:]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746cd792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vocab = pd.np.concatenate([us.city, us.st, us.state])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fc0f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array([word for word in vocab if word in wv])\n",
    "vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb8ddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_plus_state = []\n",
    "us = us.sort_values('population', ascending=False)\n",
    "for c, state, st in zip(us.city, us.state, us.st):\n",
    "    if c not in vocab:\n",
    "        continue\n",
    "    row = []\n",
    "    if state in vocab:\n",
    "        row.extend(wv[c] + wv[state])\n",
    "    else:\n",
    "        row.extend(wv[c] + wv[st])\n",
    "    city_plus_state.append(row)\n",
    "us_300D_sorted = pd.DataFrame(city_plus_state)\n",
    "del city_plus_state\n",
    "del wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9df3af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2) \n",
    "us_300D = get_data('cities_us_wordvectors')\n",
    "us_2D = pca.fit_transform(us_300D.iloc[:10, :300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb2334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这段代码已然失效,如果要看图还是需要自己画一个.\n",
    "from nlpia.data.loaders import get_data\n",
    "from nlpia.plots import offline_plotly_scatter_bubble\n",
    "df = get_data('cities_us_wordvectors_pca2_meta')\n",
    "df = df.sort_values('population', ascending=False)[:10].copy()\n",
    "df[['x', 'y']] = - df[['x', 'y']]  # <1>\n",
    "html = offline_plotly_scatter_bubble(\n",
    "    df, x='x', y='y',\n",
    "    size_col=None, text_col='name', category_col=None,\n",
    "    xscale=None, yscale=None,  # 'log' or None\n",
    "    layout={}, marker={'sizeref': 3000})\n",
    "with open(r'D:\\中间结果\\wordmap.html', 'w') as fout:\n",
    "    fout.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a4a20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
