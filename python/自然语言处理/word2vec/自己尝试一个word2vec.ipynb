{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "039c5ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you're asking me out. that's so cute. what's your name again?\tforget it.\n",
      "no, no, it's my fault we di\n"
     ]
    }
   ],
   "source": [
    "#自己训练一个word2vec,由于书中的例子无法复现没办法只能借用某个数据集自己训练一个.\n",
    "with open(r'D:\\数据集\\自然语言处理某个文章\\dialog.txt','r') as f:\n",
    "    wenzhang=f.read()\n",
    "print(wenzhang[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd55b2ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.data\n",
    " \n",
    "def splitSentence(paragraph):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(paragraph)\n",
    "    return sentences\n",
    " \n",
    "juzi=splitSentence(wenzhang)\n",
    "print(juzi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "becab583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"you're asking me out.\",\n",
       " \"that's so cute.\",\n",
       " \"what's your name again?\",\n",
       " 'forget it.',\n",
       " \"no, no, it's my fault we didn't have a proper introduction \\tcameron.\",\n",
       " 'gosh, if only we could find kat a boyfriend...\\tlet me see what i can do.',\n",
       " \"c'esc ma tete.\",\n",
       " 'this is my head\\tright.',\n",
       " 'see?',\n",
       " \"you're ready for the quiz.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "juzi[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01f2589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "fenci=[]\n",
    "for i in juzi:\n",
    "    fenci.append(tokenizer.tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f53b454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164626\n",
      "[['you', \"'re\", 'asking', 'me', 'out', '.'], ['that', \"'s\", 'so', 'cute', '.'], ['what', \"'s\", 'your', 'name', 'again', '?'], ['forget', 'it', '.'], ['no', ',', 'no', ',', 'it', \"'s\", 'my', 'fault', 'we', 'did', \"n't\", 'have', 'a', 'proper', 'introduction', 'cameron', '.'], ['gosh', ',', 'if', 'only', 'we', 'could', 'find', 'kat', 'a', 'boyfriend', '...', 'let', 'me', 'see', 'what', 'i', 'can', 'do', '.'], [\"c'esc\", 'ma', 'tete', '.'], ['this', 'is', 'my', 'head', 'right', '.'], ['see', '?'], ['you', \"'re\", 'ready', 'for', 'the', 'quiz', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(len(fenci))\n",
    "print(fenci[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff97f035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "#停用词\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42aa43a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164626\n",
      "[[\"'re\", 'asking', '.'], [\"'s\", 'cute', '.'], [\"'s\", 'name', '?'], ['forget', '.'], [',', ',', \"'s\", 'fault', \"n't\", 'proper', 'introduction', 'cameron', '.'], ['gosh', ',', 'could', 'find', 'kat', 'boyfriend', '...', 'let', 'see', '.'], [\"c'esc\", 'tete', '.'], ['head', 'right', '.'], ['see', '?'], [\"'re\", 'ready', 'quiz', '.']]\n"
     ]
    }
   ],
   "source": [
    "fenci2=[]\n",
    "for i in fenci:\n",
    "    fenci2.append([x for x in i if x not in stop_words])\n",
    "print(len(fenci2))\n",
    "print(fenci2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10f1e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model=Word2Vec(fenci2,workers=2,vector_size=300,min_count=3,window=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cfd4a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv\n",
    "word_vectors.save(\"D:/中间结果/word2vec.wordvectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc002a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load(\"D:/中间结果/word2vec.wordvectors\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25960c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('afraid', 0.8736554980278015),\n",
       " ('glad', 0.8719610571861267),\n",
       " ('starved', 0.8714357614517212),\n",
       " ('liable', 0.8464757800102234),\n",
       " ('sure', 0.846079409122467),\n",
       " ('scared', 0.8458653092384338),\n",
       " ('tired', 0.8451772928237915),\n",
       " ('hungry', 0.8444514274597168),\n",
       " ('fine', 0.8132206201553345),\n",
       " (\"'cos\", 0.8115018606185913)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#可以看到由于处理不干净并且文档太小,可能效果不好\n",
    "wv.most_similar('sorry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b1277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
